# ---- Reusable logger (handles skipped tasks gracefully) ----
STATE_MESSAGES = {
    "JOB_STATE_STOPPED":   "Job created but not yet running",
    "JOB_STATE_PENDING":   "Job is pending resource assignment",
    "JOB_STATE_RUNNING":   "Job is currently running",
    "JOB_STATE_DONE":      "Job finished successfully",
    "JOB_STATE_FAILED":    "Job failed due to an error",
    "JOB_STATE_CANCELLED": "Job was explicitly cancelled",
    "JOB_STATE_UPDATED":   "Job graph was updated",
    "JOB_STATE_DRAINING":  "Job is draining",
    "JOB_STATE_DRAINED":   "Job finished draining",
}

def log_multiple_dataflow_jobs(upstream_task_ids: list[str], **kwargs):
    ti = kwargs["ti"]
    for tid in upstream_task_ids:
        job_info = ti.xcom_pull(task_ids=tid)  # returns None if task skipped/failed early
        if not isinstance(job_info, dict):
            logging.warning("No XCom dict from %s (skipped or failed before return).", tid)
            continue
        job = job_info.get("job", {})
        job_id = job.get("id"); name = job.get("name"); state = job.get("state")
        proj = job.get("projectId"); region = job.get("location")
        logging.info("ðŸš€ [%s] name=%s id=%s state=%s (%s)", tid, name, job_id, state,
                     STATE_MESSAGES.get(state, "Unknown"))
        if job_id and proj and region:
            logging.info("    URL: https://console.cloud.google.com/dataflow/jobs/%s/%s?project=%s",
                         region, job_id, proj)
